{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bhumokaggle/competition-momentum-optimizer?scriptVersionId=198703493\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-23T03:48:25.317928Z","iopub.execute_input":"2024-09-23T03:48:25.318454Z","iopub.status.idle":"2024-09-23T03:48:25.698056Z","shell.execute_reply.started":"2024-09-23T03:48:25.318418Z","shell.execute_reply":"2024-09-23T03:48:25.697116Z"}}},{"cell_type":"code","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-09-28T22:48:46.580137Z","iopub.execute_input":"2024-09-28T22:48:46.580694Z","iopub.status.idle":"2024-09-28T22:48:47.168042Z","shell.execute_reply.started":"2024-09-28T22:48:46.58063Z","shell.execute_reply":"2024-09-28T22:48:47.166443Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n# Define a simple CNN model for CIFAR-10\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 64 * 8 * 8)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Custom optimizer: CompetingMomentum\nimport torch\nfrom torch.optim import Optimizer\n\nclass CompetingMomentum(Optimizer):\n    def __init__(self, params, lr=0.001, momentum_coefficient=0.9, K=1.0, method_type=\"magnitude_velocity\"):\n        defaults = {'lr': lr, 'momentum_coefficient': momentum_coefficient, 'K': K}\n        super(CompetingMomentum, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        for group in self.param_groups:\n            lr = group['lr']\n            momentum_coefficient = group['momentum_coefficient']\n            K = group['K']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n\n                g = p.grad\n\n                # Separate positive and negative gradients without cloning\n                g_pos = torch.clamp(g, min=0)\n                g_neg = torch.clamp(g, max=0)\n\n                # Compute magnitudes\n                mag_plus = torch.sum(g_pos)\n                mag_minus = -torch.sum(g_neg)\n\n                total_mag = mag_plus + mag_minus\n\n                if total_mag > 0:\n                    mag_plus_normalized = ((2 * mag_plus) / total_mag).pow(K)\n                    mag_minus_normalized = ((2 * mag_minus) / total_mag).pow(K)\n                    g_prime = mag_plus_normalized * g_pos + mag_minus_normalized * g_neg\n                else:\n                    g_prime = g.clone()\n\n                # Access the optimizer state\n                state = self.state[p]\n\n                # Initialize velocity if not present\n                if 'velocity' not in state:\n                    state['velocity'] = torch.zeros_like(p)\n                velocity = state['velocity']\n                \n                # Update velocity\n                velocity.mul_(momentum_coefficient).add_(g_prime)\n\n                # Update parameters using torch.no_grad()\n                with torch.no_grad():\n                    p -= lr * velocity\n    def calculate_competition_gradients(self, pos_camp, neg_camp, original_g):\n        if self.method == \"momentum_velocity_with_g_prime\":\n            magnitude_velocity_with_g_prime\n\n    def magnitude_velocity_with_g_prime(self, pos_camp, neg_camp, original_g):\n        # Compute magnitudes\n        mag_plus = torch.sum(pos_camp)\n        mag_minus = -torch.sum(neg_camp)\n        \n        # Compute total magnitude\n        total_mag = mag_plus + mag_minus\n\n        \n        if total_mag > 0:\n            mag_plus_normalized = ((2 * mag_plus) / total_mag).pow(K)\n            mag_minus_normalized = ((2 * mag_minus) / total_mag).pow(K)\n            g_prime = mag_plus_normalized * g_pos + mag_minus_normalized * g_neg\n        else:\n            g_prime = g.clone()\n              # Access the optimizer state\n                state = self.state[p]\n\n        # Initialize velocity if not present\n        if 'velocity' not in state:\n            state['velocity'] = torch.zeros_like(p)\n        velocity = state['velocity']\n                \n        # Update velocity\n        velocity.mul_(momentum_coefficient).add_(g_prime)\n\n\n\n\n\n        \n# Training function\ndef train(model, criterion, optimizer, train_loader, test_loader, epochs=10, device='cpu'):\n    model.train()\n    train_losses = []\n    val_losses = []\n\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)  # Move to the correct device\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        \n        # Average training loss for the epoch\n        avg_train_loss = epoch_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n        \n        # Validation loss\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for inputs, labels in test_loader:\n                inputs, labels = inputs.to(device), labels.to(device)  # Move to the correct device\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n        avg_val_loss = val_loss / len(test_loader)\n        val_losses.append(avg_val_loss)\n\n        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n\n    return train_losses, val_losses\n\n# Testing function\ndef test_model(model, test_loader, device='cpu'):\n    model.eval()  # Set the model to evaluation mode\n    correct = 0\n    total = 0\n    with torch.no_grad():  # Disable gradient tracking\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)  # Move to the correct device\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    accuracy = 100 * correct / total\n    print(f'Test Accuracy: {accuracy:.2f}%')\n\n# Function to plot training and validation loss\ndef plot_losses(train_losses, val_losses):\n    plt.figure(figsize=(10, 5))\n    plt.plot(train_losses, label='Training Loss', color='blue')\n    plt.plot(val_losses, label='Validation Loss', color='orange')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n# Main execution\nif __name__ == '__main__':\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # Load CIFAR-10 dataset\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n\n    # Load test dataset for validation\n    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n    \n    print(\"***************************\")\n    \n     # Initialize model, criterion, and optimizer\n    model = SimpleCNN().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = CompetingMomentum(model.parameters(), lr=0.005, momentum_coefficient=0.9, K=1)\n\n    # Train the model and get the losses\n    train_losses, val_losses = train(model, criterion, optimizer, train_loader, test_loader, epochs=100, device=device)\n\n    # Test the model\n    test_model(model, test_loader, device=device)\n\n    # Plot training and validation losses\n    plot_losses(train_losses, val_losses)\n    print(\"***************************\")\n         # Initialize model, criterion, and optimizer\n    model = SimpleCNN().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = CompetingMomentum(model.parameters(), lr=0.005, momentum_coefficient=0.5, K=1)\n\n    # Train the model and get the losses\n    train_losses, val_losses = train(model, criterion, optimizer, train_loader, test_loader, epochs=100, device=device)\n\n    # Test the model\n    test_model(model, test_loader, device=device)\n\n    # Plot training and validation losses\n    plot_losses(train_losses, val_losses)\n    print(\"***************************\")\n    \n      Initialize model, criterion, and optimizer\n    model = SimpleCNN().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = CompetingMomentum(model.parameters(), lr=0.005, momentum_coefficient=0.9, K=0.5)\n\n    # Train the model and get the losses\n    train_losses, val_losses = train(model, criterion, optimizer, train_loader, test_loader, epochs=100, device=device)\n\n    # Test the model\n    test_model(model, test_loader, device=device)\n\n    # Plot training and validation losses\n    plot_losses(train_losses, val_losses)\n    print(\"***************************\")\n         # Initialize model, criterion, and optimizer\n    model = SimpleCNN().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = CompetingMomentum(model.parameters(), lr=0.005, momentum_coefficient=0.5, K=0.5)\n\n    # Train the model and get the losses\n    train_losses, val_losses = train(model, criterion, optimizer, train_loader, test_loader, epochs=100, device=device)\n\n    # Test the model\n    test_model(model, test_loader, device=device)\n\n    # Plot training and validation losses\n    plot_losses(train_losses, val_losses)\n    print(\"***************************\")","metadata":{"execution":{"iopub.status.busy":"2024-09-28T22:48:47.171346Z","iopub.execute_input":"2024-09-28T22:48:47.172032Z","iopub.status.idle":"2024-09-28T22:49:01.47513Z","shell.execute_reply.started":"2024-09-28T22:48:47.171974Z","shell.execute_reply":"2024-09-28T22:49:01.47364Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:03<00:00, 46030850.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print(\"***************************\")\n    \n# Initialize model, criterion, and optimizer\nmodel = SimpleCNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = CompetingMomentum(model.parameters(), lr=0.01, momentum_coefficient=0.9, K=1)\n\n# Train the model and get the losses\ntrain_losses, val_losses = train(model, criterion, optimizer, train_loader, test_loader, epochs=100, device=device)\n\n# Test the model\ntest_model(model, test_loader, device=device)\n\n# Plot training and validation losses\nplot_losses(train_losses, val_losses)\n\nprint(\"***************************\")\n\n# Initialize model, criterion, and optimizer\nmodel = SimpleCNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = CompetingMomentum(model.parameters(), lr=0.01, momentum_coefficient=0.5, K=1)\n\n# Train the model and get the losses\ntrain_losses, val_losses = train(model, criterion, optimizer, train_loader, test_loader, epochs=100, device=device)\n\n# Test the model\ntest_model(model, test_loader, device=device)\n\n# Plot training and validation losses\nplot_losses(train_losses, val_losses)\n\n\nprint(\"***************************\")\n    \n# Initialize model, criterion, and optimizer\nmodel = SimpleCNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = CompetingMomentum(model.parameters(), lr=0.01, momentum_coefficient=0.9, K=0.5)\n\n# Train the model and get the losses\ntrain_losses, val_losses = train(model, criterion, optimizer, train_loader, test_loader, epochs=100, device=device)\n\n# Test the model\ntest_model(model, test_loader, device=device)\n\n# Plot training and validation losses\nplot_losses(train_losses, val_losses)\n\n\nprint(\"***************************\")\n\n# Initialize model, criterion, and optimizer\nmodel = SimpleCNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = CompetingMomentum(model.parameters(), lr=0.01, momentum_coefficient=0.5, K=0.5)\n\n# Train the model and get the losses\ntrain_losses, val_losses = train(model, criterion, optimizer, train_loader, test_loader, epochs=100, device=device)\n\n# Test the model\ntest_model(model, test_loader, device=device)\n\n# Plot training and validation losses\nplot_losses(train_losses, val_losses)\n\nprint(\"***************************\")","metadata":{"execution":{"iopub.status.busy":"2024-09-28T22:49:01.476916Z","iopub.execute_input":"2024-09-28T22:49:01.477515Z","iopub.status.idle":"2024-09-28T22:49:30.503398Z","shell.execute_reply.started":"2024-09-28T22:49:01.477467Z","shell.execute_reply":"2024-09-28T22:49:30.501112Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/2251225790.py:76: UserWarning: This overload of add_ is deprecated:\n\tadd_(Number alpha, Tensor other)\nConsider using one of the following signatures instead:\n\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1581.)\n  velocity.mul_(momentum_coefficient).add_(1 - momentum_coefficient, g_prime)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m CompetingMomentum(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, momentum_coefficient\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, K\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Train the model and get the losses\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[1;32m     26\u001b[0m test_model(model, test_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n","Cell \u001b[0;32mIn[2], line 95\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, train_loader, test_loader, epochs, device)\u001b[0m\n\u001b[1;32m     93\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     94\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 95\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     97\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":3}]}