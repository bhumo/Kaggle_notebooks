{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bhumokaggle/sgd-gradient-competition-list-magnitude?scriptVersionId=210734013\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n# Define a simple CNN model\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n        self.fc1 = nn.Linear(32*6*6, 128)\n        self.fc2 = nn.Linear(128, 10)\n        \n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = torch.relu(self.conv2(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = x.view(-1, 32*6*6)\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Function to visualize gradients\ndef visualize_gradients(g1, g2, final_grads, title=\"Gradients Visualization\"):\n    num_params = len(g1)\n    \n    fig, axs = plt.subplots(num_params, 3, figsize=(15, num_params * 2))\n    fig.suptitle(title, fontsize=16)\n\n    for i, (grad1, grad2, final_grad) in enumerate(zip(g1, g2, final_grads)):\n        if grad1 is not None:\n            axs[i, 0].hist(grad1.view(-1).cpu().numpy(), bins=50, alpha=0.7, color='blue', label='g1')\n            axs[i, 0].set_title(f'Layer {i} - g1')\n        if grad2 is not None:\n            axs[i, 1].hist(grad2.view(-1).cpu().numpy(), bins=50, alpha=0.7, color='green', label='g2')\n            axs[i, 1].set_title(f'Layer {i} - g2')\n        if final_grad is not None:\n            axs[i, 2].hist(final_grad.view(-1).cpu().numpy(), bins=50, alpha=0.7, color='red', label='Final')\n            axs[i, 2].set_title(f'Layer {i} - Final')\n\n    plt.tight_layout(rect=[0, 0, 1, 0.96])\n    plt.show()\n\n# Gradient competition logic\ndef compete_gradients(grads_tensor, competition_criterion=\"magnitude\"):\n    final_grads = []\n    # print(grads_tensor.shape)\n\n    \n    if competition_criterion == \"vote\":\n\n        # Determine the majority sign for each element across the batch\n        grad_signs = torch.sign(grads_tensor)  # Get the sign (-1, 0, +1) of each element\n        vote_sum = torch.sum(grad_signs, dim=0)  # Sum signs across the batch dimension\n        \n        # Majority sign will be positive if vote_sum > 0, negative if vote_sum < 0\n        majority_sign = torch.sign(vote_sum)\n        \n        # Compute final gradient based on majority sign\n        final_grad = torch.where(majority_sign > 0, torch.sum(torch.where(grad_signs > 0, grads_tensor, torch.zeros_like(grads_tensor)), dim=0), \n                                 torch.sum(torch.where(grad_signs < 0, grads_tensor, torch.zeros_like(grads_tensor)), dim=0))\n\n\n    if competition_criterion == \"magnitude\":\n        \n        positive_grads = torch.where(grads_tensor >= 0, grads_tensor, torch.zeros_like(grads_tensor))\n        negative_grads = torch.where(grads_tensor < 0, grads_tensor, torch.zeros_like(grads_tensor))\n\n        # Calculate the sum for positive and negative gradients across the batch dimension (dim=0)\n        pos_sum = torch.sum(positive_grads, dim=0)\n        neg_sum = torch.sum(negative_grads, dim=0)\n\n        # Compare the sums and select the gradients with the larger sum element-wise\n        final_grad = torch.where(torch.abs(pos_sum) >= torch.abs(neg_sum), pos_sum, neg_sum)\n\n\n        \n    if competition_criterion == \"magnitude_norms\":\n        \n        # Normalize gradients by their norms and compare\n        norms = torch.norm(grads_tensor, dim=0, keepdim=True)\n        normalized_grads = grads_tensor / (norms + 1e-8)  # Avoid division by zero\n        \n        # Compare normalized magnitudes\n        g_larger = torch.abs(normalized_grads[0]) > torch.abs(normalized_grads[1])\n        \n        # Final gradient based on normalized magnitude comparison\n        final_grad = torch.where(g_larger, grads_tensor[0], grads_tensor[1])\n        \n    if competition_criterion == \"disagreement\":\n        # Check for disagreement in signs across the batch and zero out if there is disagreement\n        same_sign = (torch.sign(grads_tensor[0]) == torch.sign(grads_tensor[1]))\n        final_grad = torch.where(same_sign, grads_tensor[0] + grads_tensor[1], torch.zeros_like(grads_tensor[0]))\n      \n\n    \n    \n\n          \n    final_grads.append(final_grad)\n    return final_grads\n\n\n# Function to validate the model\ndef validate_model(model, val_loader, criterion):\n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            val_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n    avg_val_loss = val_loss / len(val_loader)\n    val_accuracy = 100. * correct / total\n    return avg_val_loss, val_accuracy\n\n# Training loop with gradient competition\ndef train_with_competition(model, train_loader, val_loader, criterion, optimizer, competition_criterion=\"magnitude\", epochs=16):\n    training_losses = []\n    validation_losses = []\n    \n    for epoch in range(epochs):\n        model.train()\n        training_loss = 0\n        correct = 0\n        total = 0\n        \n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            batch_size = inputs.size(0)\n            sample_grads = [[] for _ in range(batch_size)]\n            \n            for i in range(batch_size):\n                optimizer.zero_grad()\n                output = model(inputs[i].unsqueeze(0))\n                loss = criterion(output, targets[i].unsqueeze(0))\n                loss.backward()\n                for param_idx, param in enumerate(model.parameters()):\n                    if param.grad is not None:\n                        sample_grads[i].append(param.grad.clone())\n\n            final_grads = []\n            for param_idx in range(len(sample_grads[0])):\n                grads = torch.stack([sample_grads[i][param_idx] for i in range(batch_size)], dim=0)\n                final_grad = compete_gradients(grads, competition_criterion=competition_criterion)\n                final_grads.append(final_grad[0])\n            \n            for param, final_grad in zip(model.parameters(), final_grads):\n                if final_grad is not None:\n                    param.grad = final_grad\n            \n            optimizer.step()\n            \n            # Accumulate training loss and accuracy\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            training_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n        avg_train_loss = training_loss / len(train_loader)\n        train_accuracy = 100. * correct / total\n        training_losses.append(avg_train_loss)\n\n        # Calculate validation loss and accuracy\n        val_loss, val_accuracy = validate_model(model, val_loader, criterion)\n        validation_losses.append(val_loss)\n        \n        print(f'Epoch [{epoch+1}/{epochs}] Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n        \n        # Visualize gradients for one batch in the epoch\n        # visualize_gradients(sample_grads[0], sample_grads[1], final_grads, title=f\"Gradients Visualization Epoch:{epoch}\")\n\n    # Plot training and validation losses\n    plt.figure(figsize=(10, 5))\n    plt.plot(range(1, epochs+1), training_losses, label='Training Loss')\n    plt.plot(range(1, epochs+1), validation_losses, label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Training and Validation Losses Over Epochs')\n    plt.show()\n\n\ndef test_model(model, test_loader, criterion):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n    print(f'Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {100.*correct/total:.2f}%')\n\n# Load CIFAR-10 dataset\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n])\n\ntrain_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n\ntest_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False, num_workers=2)\n\n# Initialize model, criterion, and optimizer\nmodel = SimpleCNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.5)\n\n# Train with test data as validation\ntrain_with_competition(model, train_loader, test_loader, criterion, optimizer, competition_criterion=\"magnitude\", epochs=100)\n# Test the model performance\n\ntest_model(model, test_loader, criterion)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LR = 0.0001 & Momentum=0.9","metadata":{}},{"cell_type":"code","source":"\n# Initialize model, criterion, and optimizer\nmodel = SimpleCNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\nprint(\"LR 0.0001 & Momentum = 0.9\")\n# Train the model using gradient competition\nepochs = 100\ntrain_with_competition(model, train_loader,test_loader, criterion, optimizer, competition_criterion=\"magnitude\", epochs=epochs)\ntest_model(model, test_loader, criterion)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LR = 0.0001 & Momentum = 0.5","metadata":{}},{"cell_type":"code","source":"\n# # Initialize model, criterion, and optimizer\n# model = SimpleCNN()\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.5)\n# print(\"LR 0.0001 & Momentum = 0.5\")\n# # Train the model using gradient competition\n# epochs = 100\n# train_with_competition(model, train_loader,val_loader, criterion, optimizer, competition_criterion=\"magnitude\", epochs=epochs)\n# test_model(model, test_loader, criterion)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model with zero momentum & learning rate = 0.00001","metadata":{}},{"cell_type":"code","source":"# # Initialize model, criterion, and optimizer\n# model = SimpleCNN()\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.SGD(model.parameters(), lr=0.00001, momentum=0)\n# print(\"LR 0.00001 & Momentum = 0.0\")\n# # Train the model using gradient competition\n# epochs = 100\n# train_with_competition(model, train_loader, criterion, optimizer, competition_criterion=\"magnitude\", epochs=epochs)\n# test_model(model, test_loader, criterion)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Magnitude Momemtum = 0 & learning rate = 5e-5","metadata":{}},{"cell_type":"code","source":"# # Initialize model, criterion, and optimizer\n# model = SimpleCNN()\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.SGD(model.parameters(), lr=5e-5, momentum=0)\n# print(\"LR 5e-5 & Momentum = 0.0\")\n# # Train the model using gradient competition\n# epochs = 100\n# train_with_competition(model, train_loader, criterion, optimizer, competition_criterion=\"magnitude\", epochs=epochs)\n# test_model(model, test_loader, criterion)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Magnitude momentum = 0 & learning rate = 1e-5","metadata":{}},{"cell_type":"code","source":"# # Initialize model, criterion, and optimizer\n# model = SimpleCNN()\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.SGD(model.parameters(), lr=1e-5, momentum=0)\n# print(\"LR 1e-5 & Momentum = 0.0\")\n# # Train the model using gradient competition\n# epochs = 100\n# train_with_competition(model, train_loader, criterion, optimizer, competition_criterion=\"magnitude\", epochs=epochs)\n# test_model(model, test_loader, criterion)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n","metadata":{}},{"cell_type":"code","source":"# print(grads.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# grads_example = torch.stack([sample_grads[0][0] for i in range(2)])\n# print(grads_example.shape)\n# print(grads_example)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Now lets do the unit testing for one parameter and multi-dimension\n\n#  # Compute the positive and negative gradients\n#         positive_grads = torch.where(grads_tensor >= 0, grads_tensor, torch.zeros_like(grads_tensor))\n#         negative_grads = torch.where(grads_tensor < 0, grads_tensor, torch.zeros_like(grads_tensor))\n\n#         # Calculate the sum for positive and negative gradients across the batch dimension (dim=0)\n#         pos_sum = torch.sum(positive_grads, dim=0)\n#         neg_sum = torch.sum(negative_grads, dim=0)\n\n#         # Compare the sums and select the gradients with the larger sum element-wise\n#         final_grad = torch.where(torch.abs(pos_sum) >= torch.abs(neg_sum), pos_sum, neg_sum)\n#  # Compute the positive and negative gradients\n#         positive_grads = torch.where(grads_tensor >= 0, grads_tensor, torch.zeros_like(grads_tensor))\n#         negative_grads = torch.where(grads_tensor < 0, grads_tensor, torch.zeros_like(grads_tensor))\n\n#         # Calculate the sum for positive and negative gradients across the batch dimension (dim=0)\n#         pos_sum = torch.sum(positive_grads, dim=0)\n#         neg_sum = torch.sum(negative_grads, dim=0)\n\n#         # Compare the sums and select the gradients with the larger sum element-wise\n#         final_grad = torch.where(torch.abs(pos_sum) >= torch.abs(neg_sum), pos_sum, neg_sum)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# tensor1 = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n# tensor2 = torch.tensor([[10, 20, 30], [40, 50, 60], [70, 80, 90]])\n# tensor3 = torch.tensor([[100,29,1],[11,9,4],[1,1,1]])\n\n# # Stack the two tensors along a new dimension (default dimension=0)\n# stacked_tensor = torch.stack([tensor1, tensor2, tensor3])\n\n# print(stacked_tensor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# positive10_grads = torch.where(stacked_tensor >=10, stacked_tensor, torch.zeros_like(stacked_tensor))\n# positive_grads = torch.where(stacked_tensor < 10, stacked_tensor, torch.zeros_like(stacked_tensor))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(positive10_grads)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(positive_grads)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pos = torch.sum(positive10_grads,dim=0)\n# print(pos)\n# print(torch.abs(pos))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}