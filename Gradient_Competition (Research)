{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bhumokaggle/gradient-competition?scriptVersionId=216622035\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Optimizing Machine Learning Models Using Gradient Competition Techniques  \n\nOptimizing machine learning models for multi-objective tasks in complex environments is challenging, especially when gradients derived from different tasks conflict. This report explores **Gradient Competition**, a novel approach that resolves conflicting gradients by selecting the dominant gradients based on predefined criteria. \n\nGradient Competition selectively retains the \"winning\" gradients from each batch, discarding less effective gradients before updating model weights. This technique minimizes noise, reduces conflicts, and results in more effective gradient updates. The methods are applied to the **MNIST dataset** using **Stochastic Gradient Descent (SGD)** as the primary optimizer.  \n\n---\n\n## Dataset  \n\nThe **MNIST dataset** contains 70,000 handwritten images of digits (0â€“9), with 60,000 images for training and 10,000 images for testing.  \n\n---\n\n## Baseline  \n\nThe baseline experiments use SGD and the ADAM optimizer.  \n\n| Optimizer | Learning Rate | Momentum | Test Loss | Test Accuracy (%) |  \n|-----------|---------------|----------|-----------|-------------------|  \n| SGD       | 0.001         | 0.9      | 0.7991    | 72.50             |  \n| ADAM      | 0.001         | 0.9      | 0.7469    | 74.57             |  \n\n---\n\n## Gradient Competition Methods  \n\n### 1. **Magnitude-Based Criteria**  \n\nThe **magnitude-based criteria** prioritize significant gradient directions during weight updates by focusing on gradients with the largest magnitudes.  \n\n**Algorithm Overview**:  \n1. Divide baseline gradients into positive and negative camps across batch dimensions.  \n2. Sum magnitudes in each camp.  \n3. Retain gradients from the dominant camp for weight updates.  \n\n#### Results  \n\n| Learning Rate | Momentum | Test Loss | Test Accuracy (%) |  \n|---------------|----------|-----------|-------------------|  \n| 0.0001        | 0.9      | 0.8903    | 70.95             |  \n| 0.0001        | 0.5      | 0.7629    | 74.67             |  \n| 5e-5          | 0.0      | 0.7969    | 72.85             |  \n\n**Observations**:  \n- Accuracy peaked at **74.67%** for a learning rate of 0.0001 and momentum of 0.5.  \n- Overfitting was evident from increasing validation loss after a few epochs.  \n\n---\n\n### 2. **Vote-Based Criteria**  \n\nThe **vote-based criteria** ensure gradients align with the dominant direction of the majority of signs.  \n\n**Algorithm Overview**:  \n1. Create a sign tensor (+1, 0, -1) for each gradient.  \n2. Sum signs to determine the dominant direction for each parameter.  \n3. Retain gradients aligning with the majority direction.  \n\n#### Results  \n\n| Learning Rate | Momentum | Test Loss | Test Accuracy (%) |  \n|---------------|----------|-----------|-------------------|  \n| 0.0001        | 0.9      | 2.3070    | 10.00             |  \n| 0.0001        | 0.5      | 2.4062    | 30.10             |  \n\n**Observations**:  \n- Poor performance indicates that simple majority voting is inadequate for resolving gradient conflicts.  \n\n---\n\n### 3. **Disagreement-Based Criteria**  \n\nThe **disagreement-based criteria** identify and eliminate gradients with conflicting signs.  \n\n**Algorithm Overview**:  \n1. Identify conflicting gradients (opposite signs).  \n2. Zero out conflicting gradients.  \n3. Retain consistent gradients for parameter updates.  \n\n#### Results  \n\n| Learning Rate | Momentum | Test Loss | Test Accuracy (%) |  \n|---------------|----------|-----------|-------------------|  \n| 0.0001        | 0.9      | 2.0531    | 25.68             |  \n| 0.0001        | 0.5      | 2.1416    | 26.71             |  \n\n**Observations**:  \n- Strict zeroing of gradients led to loss of useful information, resulting in low accuracy.  \n\n---\n\n### 4. **Magnitude Normalization Criteria**  \n\nThe **magnitude normalization criteria** scale gradients uniformly by normalizing them with their respective norms.  \n\n**Algorithm Overview**:  \n1. Normalize positive and negative gradients by their norms.  \n2. Retain the dominant normalized gradients for updates.  \n\n#### Results  \n\n| Learning Rate | Momentum | Test Loss | Test Accuracy (%) |  \n|---------------|----------|-----------|-------------------|  \n| 0.0001        | 0.9      | 1.3863    | 49.65             |  \n| 0.0001        | 0.5      | 1.7129    | 36.02             |  \n\n**Observations**:  \n- Marginally better performance compared to vote and disagreement criteria.  \n\n---\n\n## Dimension of Experiment Across Momentum  \n\n### 1. **Competition on Velocity (v') and Current Gradient (g')**  \n\nThis method updates gradients based on both current velocity and current gradient.  \n\n#### Results  \n\n| Learning Rate | Momentum | Test Accuracy (%) |  \n|---------------|----------|-------------------|  \n| 0.007         | 0.8      | 73.31             |  \n| 0.008         | 0.8      | 73.82             |  \n\n**Observations**:  \n- Best performance achieved with momentum of 0.8 and a learning rate of 0.008.  \n- Overfitting observed in training and validation loss.  \n\n### 2. **Competition on Current Gradient Only**  \n\nThis method resolves conflicts in current gradients while maintaining baseline momentum updates.  \n\n#### Results  \n\n| Learning Rate | Momentum | Test Accuracy (%) |  \n|---------------|----------|-------------------|  \n| 0.008         | 0.8      | 73.50             |  \n\n**Observations**:  \n- Performance slightly improved compared to baseline SGD but overfitting persisted.  \n\n---\n\n## Conclusion  \n\n- The **magnitude-based gradient competition** outperformed baseline SGD, achieving **74.67% accuracy**.  \n- Methods such as disagreement and vote performed poorly, highlighting the need for better conflict resolution strategies.  \n- The best performance was achieved using momentum 0.8 with velocity-gradient competition (**73.82% accuracy**).  \n\n**Future Work**:  \n1. Address overfitting through improved techniques.  \n2. Apply gradient competition methods to more complex datasets for validation in real-world scenarios.  \n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n# Define a simple CNN model\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n        self.fc1 = nn.Linear(32*6*6, 128)\n        self.fc2 = nn.Linear(128, 10)\n        \n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = torch.relu(self.conv2(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = x.view(-1, 32*6*6)\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Function to visualize gradients\ndef visualize_gradients(g1, g2, final_grads, title=\"Gradients Visualization\"):\n    num_params = len(g1)\n    \n    fig, axs = plt.subplots(num_params, 3, figsize=(15, num_params * 2))\n    fig.suptitle(title, fontsize=16)\n\n    for i, (grad1, grad2, final_grad) in enumerate(zip(g1, g2, final_grads)):\n        if grad1 is not None:\n            axs[i, 0].hist(grad1.view(-1).cpu().numpy(), bins=50, alpha=0.7, color='blue', label='g1')\n            axs[i, 0].set_title(f'Layer {i} - g1')\n        if grad2 is not None:\n            axs[i, 1].hist(grad2.view(-1).cpu().numpy(), bins=50, alpha=0.7, color='green', label='g2')\n            axs[i, 1].set_title(f'Layer {i} - g2')\n        if final_grad is not None:\n            axs[i, 2].hist(final_grad.view(-1).cpu().numpy(), bins=50, alpha=0.7, color='red', label='Final')\n            axs[i, 2].set_title(f'Layer {i} - Final')\n\n    plt.tight_layout(rect=[0, 0, 1, 0.96])\n    plt.show()\n\n# Gradient competition logic\ndef compete_gradients(grads_tensor, competition_criterion=\"magnitude\"):\n    final_grads = []\n    final_grad = torch.zeros_like(grads_tensor[0])\n\n    \n    if competition_criterion == \"vote\":\n#         print(\"vote\")\n        # Determine the majority sign for each element across the batch\n        grad_signs = torch.sign(grads_tensor)  # Get the sign (-1, 0, +1) of each element\n        vote_sum = torch.sum(grad_signs, dim=0)  # Sum signs across the batch dimension\n        \n        # Majority sign will be positive if vote_sum > 0, negative if vote_sum < 0\n        majority_sign = torch.sign(vote_sum)\n        \n        # Compute final gradient based on majority sign\n        final_grad = torch.where(majority_sign > 0, torch.sum(torch.where(grad_signs > 0, grads_tensor, torch.zeros_like(grads_tensor)), dim=0), \n                                 torch.sum(torch.where(grad_signs < 0, grads_tensor, torch.zeros_like(grads_tensor)), dim=0))\n\n\n    if competition_criterion == \"magnitude\":\n        \n        positive_grads = torch.where(grads_tensor >= 0, grads_tensor, torch.zeros_like(grads_tensor))\n        negative_grads = torch.where(grads_tensor < 0, grads_tensor, torch.zeros_like(grads_tensor))\n\n        # Calculate the sum for positive and negative gradients across the batch dimension (dim=0)\n        pos_sum = torch.sum(positive_grads, dim=0)\n        neg_sum = torch.sum(negative_grads, dim=0)\n\n        # Compare the sums and select the gradients with the larger sum element-wise\n        final_grad = torch.where(torch.abs(pos_sum) >= torch.abs(neg_sum), pos_sum, neg_sum)\n\n\n        \n    if competition_criterion == \"magnitude_norms\":\n        \n        # Normalize gradients by their norms and compare\n        norms = torch.norm(grads_tensor, dim=0, keepdim=True)\n        normalized_grads = grads_tensor / (norms + 1e-8)  # Avoid division by zero\n        \n        # Compare normalized magnitudes\n        g_larger = torch.abs(normalized_grads[0]) > torch.abs(normalized_grads[1])\n        \n        # Final gradient based on normalized magnitude comparison\n        final_grad = torch.where(g_larger, grads_tensor[0], grads_tensor[1])\n        \n    if competition_criterion == \"disagreement\":\n        # Check for disagreement in signs across the batch and zero out if there is disagreement\n        same_sign = (torch.sign(grads_tensor[0]) == torch.sign(grads_tensor[1]))\n        final_grad = torch.where(same_sign, grads_tensor[0] + grads_tensor[1], torch.zeros_like(grads_tensor[0]))\n      \n          \n    final_grads.append(final_grad)\n    return final_grads\n\n\n# Training loop with gradient competition\ndef train_with_competition(model, train_loader, criterion, optimizer, competition_criterion=\"magnitude\", epochs=16):\n#     global sample_grads, grads_example\n\n    model.train()\n    for epoch in range(epochs):\n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            batch_size = inputs.size(0)\n            \n            # Store gradients for each sample in a batch\n            sample_grads = [[] for _ in range(batch_size)]\n#             print(len(sample_grads))\n            \n#             for p in model.parameters():\n#                 print(f\"Parameter:{p}\")\n            for i in range(batch_size):\n                optimizer.zero_grad()\n                output = model(inputs[i].unsqueeze(0))\n                loss = criterion(output, targets[i].unsqueeze(0))\n                loss.backward()\n                for param_idx, param in enumerate(model.parameters()):\n                    if param.grad is not None:\n#                         print(f\" Param Shape {param_idx}\")\n#                         print(param.grad.clone().shape)\n                        \n                        sample_grads[i].append(param.grad.clone())\n            \n\n        # Take the simple list and slice it with 128 size for every parameter\n        \n        \n        \n        \n            # Compete gradients across the batch\n            final_grads = []\n            for param_idx in range(len(sample_grads[0])):\n\n                grads = torch.stack([sample_grads[i][param_idx] for i in range(batch_size)], dim=0)\n#                 grads2 = torch.stack([sample_grads[(i + 1) % batch_size][param_idx] for i in range(batch_size)], dim=0)\n                grads_example = grads\n#                 print(\"Gradient Shape\")\n#                 print(grads[1].shape)\n                \n                final_grad = compete_gradients(grads, competition_criterion=competition_criterion)\n                final_grads.append(final_grad[0])\n#                 final_grads.append(grads)\n#                 print(final_grad[0].shape)\n            \n            # Apply the selected gradients\n           \n            for param, final_grad in zip(model.parameters(), final_grads):\n#                     print(final_grad.shape)\n                    if final_grad is not None:\n                    \n                        param.grad = final_grad\n                        \n            \n            optimizer.step()\n        \n        # Calculate and print training loss and accuracy\n        training_loss = 0\n        correct = 0\n        total = 0\n        \n        model.eval()\n        with torch.no_grad():\n            for inputs, targets in train_loader:\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                training_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += targets.size(0)\n                correct += predicted.eq(targets).sum().item()\n        \n        print(f'Epoch [{epoch+1}/{epochs}] completed. Training Loss: {training_loss/len(train_loader):.4f}, Training Accuracy: {100.*correct/total:.2f}%')\n        visualize_gradients(sample_grads[0], sample_grads[1], final_grads, title=f\"Gradients Visualization Epoch:{epoch}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load CIFAR-10 dataset\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n])\n\ntrain_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n\ntest_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False, num_workers=2)\n\n\n# Test the model performance\ndef test_model(model, test_loader, criterion):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n    print(f'Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {100.*correct/total:.2f}%')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n# Define a simple CNN model for CIFAR-10\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 64 * 8 * 8)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Custom optimizer: CompetingMomentum\nimport torch\nfrom torch.optim import Optimizer\nimport torch\nfrom torch.optim import Optimizer\n\nclass CompetingMomentum(Optimizer):\n    def __init__(self, params, lr=0.001, momentum_coefficient=0.9, K=1.0, mode = \"standard\"):\n        self.mode = mode\n        defaults = {'lr': lr, 'momentum_coefficient': momentum_coefficient, 'K': K}\n        super(CompetingMomentum, self).__init__(params, defaults)\n\n    def competing_momentum_step(self):\n        \"\"\"\n        Applies the 'competing momentum' optimization step, which separates positive \n        and negative gradients, normalizes them, and uses a momentum-based update.\n        \"\"\"\n        for group in self.param_groups:\n            lr = group['lr']\n            momentum_coefficient = group['momentum_coefficient']\n            K = group['K']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n\n                g = p.grad.data\n\n                # Separate positive and negative gradients\n                g_pos = torch.clamp(g, min=0)\n                g_neg = torch.clamp(g, max=0)\n\n                # Compute magnitudes\n                mag_plus = torch.sum(g_pos)\n                mag_minus = -torch.sum(g_neg)\n\n                total_mag = mag_plus + mag_minus\n\n                if total_mag > 0:\n                    mag_plus_normalized = ((2 * mag_plus) / total_mag).pow(K)\n                    mag_minus_normalized = ((2 * mag_minus) / total_mag).pow(K)\n                    g_prime = mag_plus_normalized * g_pos + mag_minus_normalized * g_neg\n                else:\n                    g_prime = g.clone()\n\n                # Access optimizer state\n                state = self.state[p]\n\n                # Initialize step counter if not present\n                if 'step' not in state:\n                    state['step'] = 0\n                \n                # Increment step counter\n                state['step'] += 1\n                t = state['step']\n\n                # Initialize original velocity if not present\n                if 'original_velocity' not in state:\n                    state['original_velocity'] = torch.zeros_like(p)\n                \n                original_velocity = state['original_velocity']\n                \n                # Initialize velocity_prime if not present\n                if 'velocity_prime' not in state:\n                    state['velocity_prime'] = torch.zeros_like(p)\n                \n                velocity_prime = state['velocity_prime']\n\n                # Apply different logic for t=1 and t>1\n                if t == 1:\n                    # Use normal gradient for t = 1\n                    velocity_prime = g.clone()\n                else:\n                    # Use momentum-based updates for t > 1\n                    velocity_prime = original_velocity.clone().mul_(momentum_coefficient).add_(g_prime)\n\n                # Update original velocity in both cases\n                original_velocity.mul_(momentum_coefficient).add_(g)\n\n                # Update parameters using torch.no_grad()\n                with torch.no_grad():\n                    p -= lr * velocity_prime\n\n    def standard_momentum_step(self):\n        \"\"\"\n        Applies the standard momentum-based update without separating gradients into\n        positive and negative components.\n        \"\"\"\n        for group in self.param_groups:\n            lr = group['lr']\n            momentum_coefficient = group['momentum_coefficient']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n\n                g = p.grad.data  \n                \n                # Access optimizer state\n                state = self.state[p]\n\n                # Initialize step counter if not present\n                if 'step' not in state:\n                    state['step'] = 0\n                \n                # Increment step counter\n                state['step'] += 1\n                t = state['step']\n                \n\n                # Separate positive and negative gradients\n                g_pos = torch.clamp(g, min=0)\n                g_neg = torch.clamp(g, max=0)\n\n                # Compute magnitudes\n                mag_plus = torch.sum(g_pos)\n                mag_minus = -torch.sum(g_neg)\n\n                total_mag = mag_plus + mag_minus\n\n                if total_mag > 0:\n                    mag_plus_normalized = ((2 * mag_plus) / total_mag).pow(K)\n                    mag_minus_normalized = ((2 * mag_minus) / total_mag).pow(K)\n                    g_prime = mag_plus_normalized * g_pos + mag_minus_normalized * g_neg\n                else:\n                    g_prime = g.clone()\n                \n                # Initialize velocity_prime if not present\n                if 'velocity_prime' not in state:\n                    state['velocity_prime'] = torch.zeros_like(p)\n                \n                velocity_prime = state['velocity_prime']\n                if t == 1:\n                    velocity_prime = g.clone()\n                else:\n                    # Update velocity with momentum\n                    velocity_prime.mul_(momentum_coefficient).add_(g_prime)\n\n                # Update parameters using torch.no_grad()\n                with torch.no_grad():\n                    p -= lr * velocity_prime\n                    \n    def pure_magnitude_based_competition(self):\n           \n        \"\"\"\n        Applies the 'pure magnitude based competition' update which selects \n        gradients based on the largest sum of positive or negative gradients.\n        \"\"\"\n        for group in self.param_groups:\n            lr = group['lr']\n            momentum_coefficient = group['momentum_coefficient']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n\n                g = p.grad.data\n\n                # Separate positive and negative gradients\n                positive_grads = torch.where(g >= 0, g, torch.zeros_like(g))\n                negative_grads = torch.where(g < 0, g, torch.zeros_like(g))\n\n                # Calculate the sum for positive and negative gradients across the batch dimension\n                pos_sum = torch.sum(positive_grads)\n                neg_sum = torch.sum(negative_grads)\n\n                # Compare the sums and select the gradients with the larger sum element-wise\n                final_grad = torch.where(torch.abs(pos_sum) >= torch.abs(neg_sum), positive_grads, negative_grads)\n                p.grad.data = final_grad\n                # Access optimizer state\n                state = self.state[p]\n\n                # Initialize step counter if not present\n                if 'step' not in state:\n                    state['step'] = 0\n                \n                # Increment step counter\n                state['step'] += 1\n                t = state['step']\n\n                # Initialize velocity if not present\n                if 'velocity' not in state:\n                    state['velocity'] = torch.zeros_like(p)\n\n                velocity = state['velocity']\n                \n                # Apply different logic for t=1 and t>1\n                if t == 1:\n                    velocity = final_grad.clone()\n                else:\n                    velocity.mul_(momentum_coefficient).add_(final_grad)\n\n                # Update parameters using torch.no_grad()\n                with torch.no_grad():\n                    p -= lr * velocity\n                    \n   \n    def pure_vote_based_competition(self):\n        \"\"\"\n        Applies the 'pure vote based competition' update which determines the \n        final gradient based on the majority sign of the gradients across the batch.\n        \"\"\"\n        for group in self.param_groups:\n            lr = group['lr']\n            momentum_coefficient = group['momentum_coefficient']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n\n                grads_tensor = p.grad.data\n\n                # Get the sign (-1, 0, +1) of each element\n                grad_signs = torch.sign(grads_tensor)\n                \n                # Sum signs across the batch dimension\n                vote_sum = torch.sum(grad_signs, dim=0)\n                \n                # Majority sign will be positive if vote_sum > 0, negative if vote_sum < 0\n                majority_sign = torch.sign(vote_sum)\n\n                # Compute final gradient based on majority sign\n                final_grad = torch.where(\n                    majority_sign > 0,\n                    torch.sum(torch.where(grad_signs > 0, grads_tensor, torch.zeros_like(grads_tensor)), dim=0),\n                    torch.sum(torch.where(grad_signs < 0, grads_tensor, torch.zeros_like(grads_tensor)), dim=0)\n                )\n\n                # Access optimizer state\n                state = self.state[p]\n\n                # Initialize step counter if not present\n                if 'step' not in state:\n                    state['step'] = 0\n                \n                # Increment step counter\n                state['step'] += 1\n                t = state['step']\n\n                # Initialize velocity if not present\n                if 'velocity' not in state:\n                    state['velocity'] = torch.zeros_like(p)\n\n                velocity = state['velocity']\n                \n                # Apply different logic for t=1 and t>1\n                if t == 1:\n                    velocity = final_grad.clone()\n                else:\n                    velocity.mul_(momentum_coefficient).add_(final_grad)\n\n                # Update parameters using torch.no_grad()\n                with torch.no_grad():\n                    p -= lr * velocity\n\n                    \n                    \n    def pure_norm_based_competition(self):\n            \"\"\"\n            Applies the 'pure norm based competition' update which compares \n            normalized gradients based on their magnitudes.\n            \"\"\"\n            for group in self.param_groups:\n                lr = group['lr']\n                momentum_coefficient = group['momentum_coefficient']\n\n                for p in group['params']:\n                    if p.grad is None:\n                        continue\n\n                    grads_tensor = p.grad.data\n\n                    # Normalize gradients by their norms and compare\n                    norms = torch.norm(grads_tensor, dim=0, keepdim=True)\n                    normalized_grads = grads_tensor / (norms + 1e-8)  # Avoid division by zero\n\n                    # Compare normalized magnitudes\n                    g_larger = torch.abs(normalized_grads[0]) > torch.abs(normalized_grads[1])\n\n                    # Final gradient based on normalized magnitude comparison\n                    final_grad = torch.where(g_larger, grads_tensor[0], grads_tensor[1])\n\n                    # Access optimizer state\n                    state = self.state[p]\n\n                    # Initialize step counter if not present\n                    if 'step' not in state:\n                        state['step'] = 0\n\n                    # Increment step counter\n                    state['step'] += 1\n                    t = state['step']\n\n                    # Initialize velocity if not present\n                    if 'velocity' not in state:\n                        state['velocity'] = torch.zeros_like(p)\n\n                    velocity = state['velocity']\n\n                    # Apply different logic for t=1 and t>1\n                    if t == 1:\n                        velocity = final_grad.clone()\n                    else:\n                        velocity.mul_(momentum_coefficient).add_(final_grad)\n\n                    # Update parameters using torch.no_grad()\n                    with torch.no_grad():\n                        p -= lr * velocity\n                        \n    def pure_disagreement_based_competition(self):\n        \"\"\"\n        Applies the 'pure disagreement based competition' update which checks\n        for disagreement in signs across the batch and zeros out the gradient\n        if there is disagreement.\n        \"\"\"\n        for group in self.param_groups:\n            lr = group['lr']\n            momentum_coefficient = group['momentum_coefficient']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n\n                grads_tensor = p.grad.data\n\n                # Check for disagreement in signs across the batch\n                same_sign = (torch.sign(grads_tensor[0]) == torch.sign(grads_tensor[1]))\n                final_grad = torch.where(same_sign, grads_tensor[0] + grads_tensor[1], torch.zeros_like(grads_tensor[0]))\n\n                # Access optimizer state\n                state = self.state[p]\n\n                # Initialize step counter if not present\n                if 'step' not in state:\n                    state['step'] = 0\n                \n                # Increment step counter\n                state['step'] += 1\n                t = state['step']\n\n                # Initialize velocity if not present\n                if 'velocity' not in state:\n                    state['velocity'] = torch.zeros_like(p)\n\n                velocity = state['velocity']\n                \n                # Apply different logic for t=1 and t>1\n                if t == 1:\n                    velocity = final_grad.clone()\n                else:\n                    velocity.mul_(momentum_coefficient).add_(final_grad)\n\n                # Update parameters using torch.no_grad()\n                with torch.no_grad():\n                    p -= lr * velocity\n\n    def step(self, threshold=1):\n        \"\"\"\n        Performs a step of optimization based on the selected mode.\n\n        mode: 'competing' (default) or 'standard'\n        \"\"\"\n        mode = self.mode\n        if mode == \"competing\":\n            self.competing_momentum_step()\n        elif mode == \"standard\":\n            self.standard_momentum_step()\n        elif mode == \"pure_magnitude\":\n            self.pure_magnitude_based_competition()\n        elif mode == \"pure_vote\":\n            self.pure_vote_based_competition()\n        elif mode == \"disagrement\":\n            self.pure_disagreement_based_competition()\n        elif mode == \"pure_magnitude_threshold\":\n            self.pure_magnitude_threshold(threshold = threshold)\n        else:\n            raise ValueError(f\"Unknown mode: {mode}. Use 'competing' or 'standard'.\")\n\n# Training function\ndef train(model, criterion, optimizer, train_loader, test_loader, epochs=10, device='cpu'):\n    model.train()\n    train_losses = []\n    val_losses = []\n\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)  # Move to the correct device\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        \n        # Average training loss for the epoch\n        avg_train_loss = epoch_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n        \n        # Validation loss\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for inputs, labels in test_loader:\n                inputs, labels = inputs.to(device), labels.to(device)  # Move to the correct device\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n        avg_val_loss = val_loss / len(test_loader)\n        val_losses.append(avg_val_loss)\n\n        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n\n    return train_losses, val_losses\n\n# Testing function\ndef test_model(model, test_loader, device='cpu'):\n    model.eval()  # Set the model to evaluation mode\n    correct = 0\n    total = 0\n    with torch.no_grad():  # Disable gradient tracking\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)  # Move to the correct device\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    accuracy = 100 * correct / total\n    print(f'Test Accuracy: {accuracy:.2f}%')\n    return accuracy\n\n# Function to plot training and validation loss\ndef plot_losses(train_losses, val_losses):\n    plt.figure(figsize=(10, 5))\n    plt.plot(train_losses, label='Training Loss', color='blue')\n    plt.plot(val_losses, label='Validation Loss', color='orange')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n# Main execution\nif __name__ == '__main__':\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # Load CIFAR-10 dataset\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n\n    # Load test dataset for validation\n    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n    \n    hyperparameters = [\n     # {\"K\": 1, \"momentum\": 0.9, \"lr\": 0.001, \"mode\": \"standard\" },      \n    {\"K\": 1, \"momentum\": 0.9, \"lr\": 0.0001, \"mode\": \"competing\"},\n#     {\"K\": 1, \"momentum\": 0.8, \"lr\": 0.005, \"mode\": \"pure_magnitude\"},\n#     {\"K\": 1, \"momentum\": 0.8, \"lr\": 0.005,\"mode\": \"standard\"},\n#     {\"K\": 1, \"momentum\": 0.8, \"lr\": 0.006,\"mode\": \"pure_vote\"},\n#     {\"K\": 1, \"momentum\": 0.8, \"lr\": 0.007,\"mode\": \"disagrement\"},\n#     {\"K\": 1, \"momentum\": 0.8, \"lr\": 0.008,\"mode\": \"standard\"},\n#     {\"K\": 1, \"momentum\": 0.8, \"lr\": 0.008,\"mode\": \"competing\"},\n    # {\"K\": 1, \"momentum\": 0.5, \"lr\": 0.0001,\"mode\": \"pure_magnitude\"},\n#     {\"K\": 1, \"momentum\": 0.8, \"lr\": 0.008,\"mode\": \"pure_vote\"},\n#     {\"K\": 1, \"momentum\": 0.8, \"lr\": 0.008,\"mode\": \"disagrement\"},\n        \n]\n    \n    \n\n    # Loop through each hyperparameter combination\n    for params in hyperparameters:\n        K = params['K']\n        momentum = params['momentum']\n        lr = params['lr']\n        mode = params['mode']\n\n        # Initialize model, criterion, and optimizer\n        model = SimpleCNN().to(device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = CompetingMomentum(model.parameters(), lr=lr, momentum_coefficient=momentum, K=K, mode = mode)\n\n        # Train the model and get the losses\n        train_losses, val_losses = train(model, criterion, optimizer, train_loader, test_loader, epochs=100, device=device)\n\n        # Test the model and get accuracy\n        accuracy = test_model(model, test_loader, device=device)\n        print(f\"Test Accuracy: {accuracy:.2f}% for K={K}, Momentum={momentum}, lr={lr},mode={mode}\")\n\n#         # Save results in text file\n#         with open(f'logs_K{K}_momentum{momentum}_lr{lr}.txt', 'w') as f:\n#             f.write(f'Test Accuracy: {accuracy:.2f}%\\n\")\n\n        # Plot training and validation losses\n        print(f\"K{K}_momentum{momentum}_lr{lr}_mode{mode}\")\n        plot_losses(train_losses, val_losses)\n \n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}