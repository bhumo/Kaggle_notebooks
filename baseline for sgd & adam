{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bhumokaggle/baseline-for-sgd-adam?scriptVersionId=202580819\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-22T04:30:54.045889Z","iopub.execute_input":"2024-10-22T04:30:54.046344Z","iopub.status.idle":"2024-10-22T04:30:54.440537Z","shell.execute_reply.started":"2024-10-22T04:30:54.046302Z","shell.execute_reply":"2024-10-22T04:30:54.439612Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ADAM without gradient competition","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import random_split\nimport matplotlib.pyplot as plt\n\n# Define a simple CNN model\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n        self.fc1 = nn.Linear(32*6*6, 128)\n        self.fc2 = nn.Linear(128, 10)\n        \n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = torch.relu(self.conv2(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = x.view(-1, 32*6*6)\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Load CIFAR-10 dataset\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n])\n\ntrain_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_size = int(0.8 * len(train_set))\nval_size = len(train_set) - train_size\ntrain_subset, val_subset = random_split(train_set, [train_size, val_size])\n\n# Create DataLoaders for the new training and validation sets\ntrain_loader = torch.utils.data.DataLoader(train_subset, batch_size=128, shuffle=True, num_workers=2)\nval_loader = torch.utils.data.DataLoader(val_subset, batch_size=128, shuffle=False, num_workers=2)\n\ntest_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False, num_workers=2)\n\n# Initialize model, criterion, and optimizer\nmodel = SimpleCNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\ndef train(model, train_loader, val_loader, criterion, optimizer, epochs=100):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    train_losses = []\n    val_losses = []\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n        # Calculate training loss\n        epoch_train_loss = running_loss / len(train_loader)\n        train_losses.append(epoch_train_loss)\n\n        # Validation loss calculation\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                val_loss += loss.item()\n\n        epoch_val_loss = val_loss / len(val_loader)\n        val_losses.append(epoch_val_loss)\n        print(f'Epoch [{epoch+1}/{epochs}] - Training Loss: {epoch_train_loss:.4f}, Accuracy: {100.*correct/total:.2f}%, Validation Loss: {epoch_val_loss:.4f}')\n\n    # Plot Training and Validation Loss at the end of training\n    plt.figure(figsize=(8, 6))\n    plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')\n    plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss per Epoch')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# Test the model performance\ndef test(model, test_loader, criterion):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n    test_loss /= len(test_loader)\n    accuracy = 100. * correct / total\n    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n\n# Train the model for 100 epochs\ntrain(model, train_loader, val_loader, criterion, optimizer, epochs=200)\n\n# Evaluate the model on the test dataset\ntest(model, test_loader, criterion)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:30:54.442921Z","iopub.execute_input":"2024-10-22T04:30:54.443552Z","iopub.status.idle":"2024-10-22T04:32:09.838733Z","shell.execute_reply.started":"2024-10-22T04:30:54.443497Z","shell.execute_reply":"2024-10-22T04:32:09.837148Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SGD without gradient compition","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import random_split\n\n# Define a simple CNN model\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n        self.fc1 = nn.Linear(32*6*6, 128)\n        self.fc2 = nn.Linear(128, 10)\n        \n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = torch.relu(self.conv2(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = x.view(-1, 32*6*6)\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Training loop with loss tracking and plotting\ndef train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, epochs=20, device='cuda'):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.train()\n    \n    train_losses = []\n    val_losses = []\n    \n    for epoch in range(epochs):\n        running_train_loss = 0.0\n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            running_train_loss += loss.item()\n        \n        avg_train_loss = running_train_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n        \n        # Evaluate on the validation set\n        model.eval()\n        running_val_loss = 0.0\n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                running_val_loss += loss.item()\n        \n        avg_val_loss = running_val_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n        \n        print(f'Epoch [{epoch+1}/{epochs}] completed. Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n    \n    return train_losses, val_losses\n\n# Load CIFAR-10 dataset\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n])\n\ndataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n\n# Split dataset into training and validation sets\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_set, val_set = random_split(dataset, [train_size, val_size])\n\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\nval_loader = torch.utils.data.DataLoader(val_set, batch_size=100, shuffle=False, num_workers=2)\n\n# Load test dataset\ntest_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False, num_workers=2)\n\n# Initialize model, criterion, and optimizer\nmodel = SimpleCNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n# Train the model and get losses\ntrain_losses, val_losses = train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, epochs=200, device='cuda')\n\n# Test the model performance\ndef test_model(model, test_loader, criterion, device='cuda'):\n    model.to(device)\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n    print(f'Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {100.*correct/total:.2f}%')\n\ntest_model(model, test_loader, criterion, device='cuda')\n\n# Plot training and validation loss\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Training Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss Over Epochs')\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T04:32:09.840224Z","iopub.status.idle":"2024-10-22T04:32:09.840621Z","shell.execute_reply.started":"2024-10-22T04:32:09.840427Z","shell.execute_reply":"2024-10-22T04:32:09.840447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}