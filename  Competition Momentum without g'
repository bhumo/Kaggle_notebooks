{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bhumokaggle/competition-momentum-without-g?scriptVersionId=200713558\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-23T03:48:25.317928Z","iopub.execute_input":"2024-09-23T03:48:25.318454Z","iopub.status.idle":"2024-09-23T03:48:25.698056Z","shell.execute_reply.started":"2024-09-23T03:48:25.318418Z","shell.execute_reply":"2024-09-23T03:48:25.697116Z"}}},{"cell_type":"code","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:19:58.585918Z","iopub.execute_input":"2024-10-13T05:19:58.586774Z","iopub.status.idle":"2024-10-13T05:19:59.01087Z","shell.execute_reply.started":"2024-10-13T05:19:58.586718Z","shell.execute_reply":"2024-10-13T05:19:59.009869Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n# Define a simple CNN model for CIFAR-10\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 64 * 8 * 8)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Custom optimizer: CompetingMomentum\nimport torch\nfrom torch.optim import Optimizer\n\nclass CompetingMomentum(Optimizer):\n    def __init__(self, params, lr=0.001, momentum_coefficient=0.9, K=1.0):\n        defaults = {'lr': lr, 'momentum_coefficient': momentum_coefficient, 'K': K}\n        super(CompetingMomentum, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        for group in self.param_groups:\n            lr = group['lr']\n            momentum_coefficient = group['momentum_coefficient']\n            K = group['K']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n\n                g = p.grad.data\n\n                # Separate positive and negative gradients without cloning\n                g_pos = torch.clamp(g, min=0)\n                g_neg = torch.clamp(g, max=0)\n\n                # Compute magnitudes\n                mag_plus = torch.sum(g_pos)\n                mag_minus = -torch.sum(g_neg)\n\n                total_mag = mag_plus + mag_minus\n\n                if total_mag > 0:\n                    mag_plus_normalized = ((2 * mag_plus) / total_mag).pow(K)\n                    mag_minus_normalized = ((2 * mag_minus) / total_mag).pow(K)\n                    g_prime = mag_plus_normalized * g_pos + mag_minus_normalized * g_neg\n                else:\n                    g_prime = g.clone()\n\n                # Access the optimizer state\n                state = self.state[p]\n\n                # Initialize step counter if not present\n                if 'step' not in state:\n                    state['step'] = 0\n                \n                # Increment step counter\n                state['step'] += 1\n                t = state['step']\n\n                # Initialize original velocity if not present\n                if 'original_velocity' not in state:\n                    state['original_velocity'] = torch.zeros_like(p)\n                \n                original_velocity = state['original_velocity']\n                \n                # Initialize velocity_prime if not present\n                if 'velocity_prime' not in state:\n                    state['velocity_prime'] = torch.zeros_like(p)\n                \n                velocity_prime = state['velocity_prime']\n\n                # Apply different logic for t=1 and t>1\n                if t == 1:\n                    # Use normal gradient for t = 1\n                    velocity_prime = g_prime.clone()\n                else:\n                    # Use momentum-based updates for t > 1\n                    velocity_prime = original_velocity.clone().mul_(momentum_coefficient).add_(g_prime)\n\n                # Update original velocity in both cases\n                original_velocity.mul_(momentum_coefficient).add_(g)\n\n                # Update parameters using torch.no_grad()\n                with torch.no_grad():\n                    p -= lr * velocity_prime\n\n# Training function\ndef train(model, criterion, optimizer, train_loader, test_loader, epochs=10, device='cpu'):\n    model.train()\n    train_losses = []\n    val_losses = []\n\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)  # Move to the correct device\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        \n        # Average training loss for the epoch\n        avg_train_loss = epoch_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n        \n        # Validation loss\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for inputs, labels in test_loader:\n                inputs, labels = inputs.to(device), labels.to(device)  # Move to the correct device\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n        avg_val_loss = val_loss / len(test_loader)\n        val_losses.append(avg_val_loss)\n\n        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n\n    return train_losses, val_losses\n\n# Testing function\ndef test_model(model, test_loader, device='cpu'):\n    model.eval()  # Set the model to evaluation mode\n    correct = 0\n    total = 0\n    with torch.no_grad():  # Disable gradient tracking\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)  # Move to the correct device\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    accuracy = 100 * correct / total\n    print(f'Test Accuracy: {accuracy:.2f}%')\n    return accuracy\n\n# Function to plot training and validation loss\ndef plot_losses(train_losses, val_losses):\n    plt.figure(figsize=(10, 5))\n    plt.plot(train_losses, label='Training Loss', color='blue')\n    plt.plot(val_losses, label='Validation Loss', color='orange')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n# Main execution\nif __name__ == '__main__':\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # Load CIFAR-10 dataset\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n\n    # Load test dataset for validation\n    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n    \n    hyperparameters = [\n     {\"K\": 1, \"momentum\": 0.9, \"lr\": 0.001},      \n    {\"K\": 1, \"momentum\": 0.8, \"lr\": 0.0001},\n    {\"K\": 1, \"momentum\": 0.9, \"lr\": 0.0001},\n    {\"K\": 1, \"momentum\": 0.8, \"lr\": 0.005},\n    {\"K\": 1, \"momentum\": 0.8, \"lr\": 0.005},\n    {\"K\": 1, \"momentum\": 0.8, \"lr\": 0.006},\n    {\"K\": 1, \"momentum\": 0.8, \"lr\": 0.007},\n    {\"K\": 1, \"momentum\": 0.8, \"lr\": 0.008},\n]\n    \n    \n\n    # Loop through each hyperparameter combination\n    for params in hyperparameters:\n        K = params['K']\n        momentum = params['momentum']\n        lr = params['lr']\n\n        # Initialize model, criterion, and optimizer\n        model = SimpleCNN().to(device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = CompetingMomentum(model.parameters(), lr=lr, momentum_coefficient=momentum, K=K)\n\n        # Train the model and get the losses\n        train_losses, val_losses = train(model, criterion, optimizer, train_loader, test_loader, epochs=100, device=device)\n\n        # Test the model and get accuracy\n        accuracy = test_model(model, test_loader, device=device)\n        print(f\"Test Accuracy: {accuracy:.2f}% for K={K}, Momentum={momentum}, lr={lr}\")\n\n#         # Save results in text file\n#         with open(f'logs_K{K}_momentum{momentum}_lr{lr}.txt', 'w') as f:\n#             f.write(f'Test Accuracy: {accuracy:.2f}%\\n\")\n\n        # Plot training and validation losses\n        print(f\"K{K}_momentum{momentum}_lr{lr}\")\n        plot_losses(train_losses, val_losses)\n \n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:19:59.012978Z","iopub.execute_input":"2024-10-13T05:19:59.013494Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}