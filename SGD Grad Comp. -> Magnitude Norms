{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bhumokaggle/sgd-grad-comp-magnitude-norms?scriptVersionId=195987855\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-09T03:54:48.32588Z","iopub.execute_input":"2024-09-09T03:54:48.326338Z","iopub.status.idle":"2024-09-09T03:54:48.735935Z","shell.execute_reply.started":"2024-09-09T03:54:48.326295Z","shell.execute_reply":"2024-09-09T03:54:48.734611Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Magnitude Norms","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n# Define a simple CNN model\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n        self.fc1 = nn.Linear(32*6*6, 128)\n        self.fc2 = nn.Linear(128, 10)\n        \n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = torch.relu(self.conv2(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = x.view(-1, 32*6*6)\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Function to visualize gradients\ndef visualize_gradients(g1, g2, final_grads, title=\"Gradients Visualization\"):\n    num_params = len(g1)\n    \n    fig, axs = plt.subplots(num_params, 3, figsize=(15, num_params * 2))\n    fig.suptitle(title, fontsize=16)\n\n    for i, (grad1, grad2, final_grad) in enumerate(zip(g1, g2, final_grads)):\n        if grad1 is not None:\n            axs[i, 0].hist(grad1.view(-1).cpu().numpy(), bins=50, alpha=0.7, color='blue', label='g1')\n            axs[i, 0].set_title(f'Layer {i} - g1')\n        if grad2 is not None:\n            axs[i, 1].hist(grad2.view(-1).cpu().numpy(), bins=50, alpha=0.7, color='green', label='g2')\n            axs[i, 1].set_title(f'Layer {i} - g2')\n        if final_grad is not None:\n            axs[i, 2].hist(final_grad.view(-1).cpu().numpy(), bins=50, alpha=0.7, color='red', label='Final')\n            axs[i, 2].set_title(f'Layer {i} - Final')\n\n    plt.tight_layout(rect=[0, 0, 1, 0.96])\n    plt.show()\n\n# Gradient competition logic\ndef compete_gradients(grads_tensor, competition_criterion=\"magnitude\"):\n    final_grads = []\n    final_grad = torch.zeros_like(grads_tensor[0])\n\n    \n    if competition_criterion == \"vote\":\n#         print(\"vote\")\n        # Determine the majority sign for each element across the batch\n        grad_signs = torch.sign(grads_tensor)  # Get the sign (-1, 0, +1) of each element\n        vote_sum = torch.sum(grad_signs, dim=0)  # Sum signs across the batch dimension\n        \n        # Majority sign will be positive if vote_sum > 0, negative if vote_sum < 0\n        majority_sign = torch.sign(vote_sum)\n        \n        # Compute final gradient based on majority sign\n        final_grad = torch.where(majority_sign > 0, torch.sum(torch.where(grad_signs > 0, grads_tensor, torch.zeros_like(grads_tensor)), dim=0), \n                                 torch.sum(torch.where(grad_signs < 0, grads_tensor, torch.zeros_like(grads_tensor)), dim=0))\n\n\n    if competition_criterion == \"magnitude\":\n        \n        positive_grads = torch.where(grads_tensor >= 0, grads_tensor, torch.zeros_like(grads_tensor))\n        negative_grads = torch.where(grads_tensor < 0, grads_tensor, torch.zeros_like(grads_tensor))\n\n        # Calculate the sum for positive and negative gradients across the batch dimension (dim=0)\n        pos_sum = torch.sum(positive_grads, dim=0)\n        neg_sum = torch.sum(negative_grads, dim=0)\n\n        # Compare the sums and select the gradients with the larger sum element-wise\n        final_grad = torch.where(torch.abs(pos_sum) >= torch.abs(neg_sum), pos_sum, neg_sum)\n\n\n        \n    if competition_criterion == \"magnitude_norms\":\n        \n        # Normalize gradients by their norms and compare\n        norms = torch.norm(grads_tensor, dim=0, keepdim=True)\n        normalized_grads = grads_tensor / (norms + 1e-8)  # Avoid division by zero\n        print(norms.shape)\n        print(normalized_grads.shape)\n        print(f\"1. {normalized_grads[0].shape}\")\n        print(f\"1. {normalized_grads[1].shape}\")\n        # Compare normalized magnitudes\n        g_larger = torch.abs(normalized_grads[0]) > torch.abs(normalized_grads[1])\n        \n        # Final gradient based on normalized magnitude comparison\n        final_grad = torch.where(g_larger, grads_tensor[0], grads_tensor[1])\n        \n    if competition_criterion == \"disagreement\":\n        # Check for disagreement in signs across the batch and zero out if there is disagreement\n        same_sign = (torch.sign(grads_tensor[0]) == torch.sign(grads_tensor[1]))\n        final_grad = torch.where(same_sign, grads_tensor[0] + grads_tensor[1], torch.zeros_like(grads_tensor[0]))\n      \n          \n    final_grads.append(final_grad)\n    return final_grads\n\n\n# Training loop with gradient competition\ndef train_with_competition(model, train_loader, criterion, optimizer, competition_criterion=\"magnitude\", epochs=16):\n#     global sample_grads, grads_example\n\n    model.train()\n    for epoch in range(epochs):\n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            batch_size = inputs.size(0)\n            \n            # Store gradients for each sample in a batch\n            sample_grads = [[] for _ in range(batch_size)]\n#             print(len(sample_grads))\n            \n#             for p in model.parameters():\n#                 print(f\"Parameter:{p}\")\n            for i in range(batch_size):\n                optimizer.zero_grad()\n                output = model(inputs[i].unsqueeze(0))\n                loss = criterion(output, targets[i].unsqueeze(0))\n                loss.backward()\n                for param_idx, param in enumerate(model.parameters()):\n                    if param.grad is not None:\n#                         print(f\" Param Shape {param_idx}\")\n#                         print(param.grad.clone().shape)\n                        \n                        sample_grads[i].append(param.grad.clone())\n            \n\n        # Take the simple list and slice it with 128 size for every parameter\n        \n        \n        \n        \n            # Compete gradients across the batch\n            final_grads = []\n            for param_idx in range(len(sample_grads[0])):\n\n                grads = torch.stack([sample_grads[i][param_idx] for i in range(batch_size)], dim=0)\n#                 grads2 = torch.stack([sample_grads[(i + 1) % batch_size][param_idx] for i in range(batch_size)], dim=0)\n                grads_example = grads\n#                 print(\"Gradient Shape\")\n#                 print(grads[1].shape)\n                \n                final_grad = compete_gradients(grads, competition_criterion=competition_criterion)\n                final_grads.append(final_grad[0])\n#                 final_grads.append(grads)\n#                 print(final_grad[0].shape)\n            \n            # Apply the selected gradients\n           \n            for param, final_grad in zip(model.parameters(), final_grads):\n#                     print(final_grad.shape)\n                    if final_grad is not None:\n                    \n                        param.grad = final_grad\n                        \n            \n            optimizer.step()\n        \n        # Calculate and print training loss and accuracy\n        training_loss = 0\n        correct = 0\n        total = 0\n        \n        model.eval()\n        with torch.no_grad():\n            for inputs, targets in train_loader:\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                training_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += targets.size(0)\n                correct += predicted.eq(targets).sum().item()\n        \n        print(f'Epoch [{epoch+1}/{epochs}] completed. Training Loss: {training_loss/len(train_loader):.4f}, Training Accuracy: {100.*correct/total:.2f}%')\n        visualize_gradients(sample_grads[0], sample_grads[1], final_grads, title=f\"Gradients Visualization Epoch:{epoch}\")\n# Load CIFAR-10 dataset\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n])\n\ntrain_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n\ntest_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False, num_workers=2)\n\n\n# Test the model performance\ndef test_model(model, test_loader, criterion):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n    print(f'Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {100.*correct/total:.2f}%')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T23:18:13.766Z","iopub.execute_input":"2024-09-09T23:18:13.767122Z","iopub.status.idle":"2024-09-09T23:18:15.725067Z","shell.execute_reply.started":"2024-09-09T23:18:13.767062Z","shell.execute_reply":"2024-09-09T23:18:15.72389Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LR = 0.0001 & Momentum = 0.9","metadata":{}},{"cell_type":"code","source":"\n# Initialize model, criterion, and optimizer\nmodel = SimpleCNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\nprint(\"LR 0.0001 & Momentum = 0.9\")\n# Train the model using gradient competition\nepochs = 1\ntrain_with_competition(model, train_loader, criterion, optimizer, competition_criterion=\"magnitude_norms\", epochs=epochs)\ntest_model(model, test_loader, criterion)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T23:18:18.522205Z","iopub.execute_input":"2024-09-09T23:18:18.522965Z","iopub.status.idle":"2024-09-09T23:18:20.778544Z","shell.execute_reply.started":"2024-09-09T23:18:18.522884Z","shell.execute_reply":"2024-09-09T23:18:20.776747Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Initialize model, criterion, and optimizer\nmodel = SimpleCNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.5)\nprint(\"LR 0.0001 & Momentum = 0.5\")\n# Train the model using gradient competition\nepochs = 100\ntrain_with_competition(model, train_loader, criterion, optimizer, competition_criterion=\"magnitude_norms\", epochs=epochs)\ntest_model(model, test_loader, criterion)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T03:57:43.308957Z","iopub.execute_input":"2024-09-09T03:57:43.309479Z","iopub.status.idle":"2024-09-09T03:57:46.335598Z","shell.execute_reply.started":"2024-09-09T03:57:43.30943Z","shell.execute_reply":"2024-09-09T03:57:46.333852Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Initialize model, criterion, and optimizer\n# model = SimpleCNN()\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.SGD(model.parameters(), lr=0.00001, momentum=0)\n# print(\"LR 0.00001 & Momentum = 0\")\n# # Train the model using gradient competition\n# epochs = 100\n# train_with_competition(model, train_loader, criterion, optimizer, competition_criterion=\"magnitude_norms\", epochs=epochs)\n# test_model(model, test_loader, criterion)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T03:57:46.336651Z","iopub.status.idle":"2024-09-09T03:57:46.337139Z","shell.execute_reply.started":"2024-09-09T03:57:46.336895Z","shell.execute_reply":"2024-09-09T03:57:46.336919Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Initialize model, criterion, and optimizer\n# model = SimpleCNN()\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.SGD(model.parameters(), lr=5e-5, momentum=0)\n# print(\"LR 5e-5 & Momentum = 0\")\n# # Train the model using gradient competition\n# epochs = 100\n# train_with_competition(model, train_loader, criterion, optimizer, competition_criterion=\"magnitude_norms\", epochs=epochs)\n# test_model(model, test_loader, criterion)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T03:57:46.338516Z","iopub.status.idle":"2024-09-09T03:57:46.338988Z","shell.execute_reply.started":"2024-09-09T03:57:46.338766Z","shell.execute_reply":"2024-09-09T03:57:46.338789Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Initialize model, criterion, and optimizer\n# model = SimpleCNN()\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.SGD(model.parameters(), lr=1e-5, momentum=0)\n# print(\"LR 1e-5 & Momentum = 0\")\n# # Train the model using gradient competition\n# epochs = 100\n# train_with_competition(model, train_loader, criterion, optimizer, competition_criterion=\"magnitude_norms\", epochs=epochs)\n# test_model(model, test_loader, criterion)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T03:57:46.341191Z","iopub.status.idle":"2024-09-09T03:57:46.341637Z","shell.execute_reply.started":"2024-09-09T03:57:46.341413Z","shell.execute_reply":"2024-09-09T03:57:46.341435Z"},"trusted":true},"outputs":[],"execution_count":null}]}