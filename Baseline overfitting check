{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n# Define a simple CNN model\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n        self.fc1 = nn.Linear(32*6*6, 128)\n        self.fc2 = nn.Linear(128, 10)\n        \n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = torch.relu(self.conv2(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = x.view(-1, 32*6*6)\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Load CIFAR-10 dataset\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n])\n\n# Use full training set as training data\ntrain_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n\n# Use test set as validation set\ntest_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\nval_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False, num_workers=2)\n\n# Initialize model and criterion\nmodel = SimpleCNN()\ncriterion = nn.CrossEntropyLoss()\n\n# Hyperparameter search function for Adam and SGD optimizers\ndef hyperparameter_search(model, train_loader, val_loader, optimizer_name='adam', epochs=200, learning_rates=[0.001, 0.01]):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    for lr in learning_rates:\n        # Reinitialize model for each learning rate test\n        model = SimpleCNN().to(device)\n        \n        # Initialize optimizer\n        if optimizer_name == 'adam':\n            optimizer = optim.Adam(model.parameters(), lr=lr)\n        elif optimizer_name == 'sgd':\n            optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n\n        train_losses, val_losses = [], []\n\n        for epoch in range(epochs):\n            model.train()\n            running_loss = 0.0\n            correct_train = 0\n            total_train = 0\n            for inputs, targets in train_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total_train += targets.size(0)\n                correct_train += predicted.eq(targets).sum().item()\n\n            epoch_train_loss = running_loss / len(train_loader)\n            train_losses.append(epoch_train_loss)\n            train_accuracy = 100. * correct_train / total_train\n\n            # Validation phase using test set as validation set\n            model.eval()\n            val_loss = 0.0\n            correct_val = 0\n            total_val = 0\n            with torch.no_grad():\n                for inputs, targets in val_loader:\n                    inputs, targets = inputs.to(device), targets.to(device)\n                    outputs = model(inputs)\n                    loss = criterion(outputs, targets)\n                    val_loss += loss.item()\n                    _, predicted = outputs.max(1)\n                    total_val += targets.size(0)\n                    correct_val += predicted.eq(targets).sum().item()\n\n            epoch_val_loss = val_loss / len(val_loader)\n            val_losses.append(epoch_val_loss)\n            val_accuracy = 100. * correct_val / total_val\n\n            print(f'Epoch [{epoch+1}/{epochs}] - LR: {lr} - Train Loss: {epoch_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {epoch_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n\n        # Plot Training vs Validation Loss\n        plt.plot(train_losses, label=f'{optimizer_name.upper()} LR={lr} Train Loss')\n        plt.plot(val_losses, label=f'{optimizer_name.upper()} LR={lr} Val Loss')\n    \n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title(f'{optimizer_name.upper()} Training and Validation Loss over Epochs')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# Run hyperparameter search for Adam\nhyperparameter_search(model, train_loader, val_loader, optimizer_name='adam', epochs=200, learning_rates=[0.001, 0.0001])\n\n# Run hyperparameter search for SGD\nhyperparameter_search(model, train_loader, val_loader, optimizer_name='sgd', epochs=200, learning_rates=[0.01, 0.001])\n\n# Test the model performance (using the test set)\ndef test(model, test_loader, criterion):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n    test_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n    test_loss /= len(test_loader)\n    accuracy = 100. * correct / total\n    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n\n# Evaluate the model on the test dataset\ntest(model, val_loader, criterion)  # using the test set (val_loader) as validation set\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}