{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset from the given URL and pass it to the read_csv function to extract the table and store\n# it in the df\nurl = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\ncolumns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndf = pd.read_csv(url, header=None, names=columns)\n\n# Drop null values of the data frame\ndf = df.dropna()\n\n# Split the data into features and target variable\nX = df.drop('MEDV', axis=1).values\ny = df['MEDV'].values\n\n# Feature Scaling (Standardization) this is added to make sure that the data is not overflowing and \n\nmean = X.mean(axis=0)\nstd = X.std(axis=0)\nX = (X - mean) / std\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)\n\n# Add a bias column (intercept term)\nX_train_bias = np.c_[np.ones(X_train.shape[0]), X_train]\nX_test_bias = np.c_[np.ones(X_test.shape[0]), X_test]\n\ndef closed_form_solution(X, y):\n    \"\"\"Compute the parameters using the closed-form solution.\"\"\"\n    X_transpose = np.dot(X.T, X)\n    X_transpose_inv = np.linalg.inv(X_transpose)\n    X_T_dot_y = np.dot(X.T, y)\n    return np.dot(X_transpose_inv, X_T_dot_y)\n\n# Compute weights using closed-form solution\nweights_cf = closed_form_solution(X_train_bias, y_train)\n\ndef predict(X, w):\n    return np.dot(X, w)\n\n# Predictions on test set using closed-form solution\ny_pred_cf = predict(X_test_bias, weights_cf)\n\n# Compute Mean Squared Error (MSE) for closed-form solution\ndef mean_squared_error_manual(y_true, y_pred):\n    \"\"\" This funtion is used to calculate the mean square error, It takes the predicted y labels and ground truth labels y_true\"\"\"\n    return np.mean((y_true - y_pred) ** 2)\n\nprint(\" Ans 1. Closed-Form Solution Implementation\")\nmse_cf = mean_squared_error_manual(y_test, y_pred_cf)\nprint(f\"Closed-Form Solution Mean Squared Error: {mse_cf:.2f}\")\n\n# Gradient Descent Implementation\ndef gradient_descent(X, y, learning_rate=0.01, iterations=1000):\n    \"\"\"The function takes the input X and the ground truth y with learning rate and number of iterations Perform gradient descent optimization to find the parameters.\"\"\"\n    m, n = X.shape\n    w = np.zeros(n)\n    print(\"W:\"+ str(w.shape))\n    print(\"X:\" + str(X.shape))\n    print(\"Y:\"+str(y.shape))\n    cost_buffer = []\n    \n    for epochs in range(iterations):\n        predictions_y = np.dot(X, w)\n        predictions_y = X.dot(w)\n        # print(\"*****\")\n        # print(predictions_y.shape)\n        diff = predictions_y - y\n        # print(diff.shape)\n        # print(type(X))\n        # print(type(w))\n        x2 = np.dot(X.T, diff)\n        # print(x2.shape)\n        gradient = (1/m) * np.dot(X.T, diff)\n        w -= learning_rate * gradient\n        mse = (1/(2*m)) * np.sum((np.dot(X, w) - y) ** 2)\n        # print(mse)\n        cost_buffer.append(mse)\n    \n    return w, cost_buffer\n\n\n# Visualization for Closed Form Solution\nplt.scatter(X_test[:, 5], y_test, color='blue', label='Actual/Ground Truth')  \nplt.scatter(X_test[:, 5], y_pred_cf, color='red', label='Predicted (Closed-Form)')\nplt.xlabel('Number of Rooms (RM)') # 'RM' (average number of rooms per dwelling)\nplt.ylabel('Median Value (MEDV)')\nplt.legend()\nplt.title('Comparison of Closed-Form and Ground Truth Predictions')\nplt.show()\n\nprint(\"Ans 2: Gradient Descent Implementation\")\n# Compute weights using gradient descent\nweights_gd, cost_history = gradient_descent(X_train_bias, y_train, learning_rate=0.01, iterations=1000)\n\n# Predictions on test set using gradient descent\ny_pred_gd = predict(X_test_bias, weights_gd)\n\n\n# Compute Mean Squared Error (MSE) for gradient descent\nmse_gd = mean_squared_error_manual(y_test, y_pred_gd)\nprint(f\"Gradient Descent Mean Squared Error: {mse_gd:.2f}\")\n\n# Plot cost function convergence\nplt.plot(range(len(cost_history)), cost_history, color='blue')\nplt.xlabel('Epochs')\nplt.ylabel('Cost Function')\nplt.title('Gradient Descent Convergence')\nplt.show()\n\n# Visualization for Gradient Descent Solution\nplt.scatter(X_test[:, 5], y_test, color='blue', label='Actual/Ground Truth')  \nplt.scatter(X_test[:, 5], y_pred_gd, color='orange', label='Predicted (Gradient Descent)')\nplt.xlabel('Number of Rooms (RM)') # 'RM' (average number of rooms per dwelling)\nplt.ylabel('Median Value (MEDV)')\nplt.legend()\nplt.title('Comparison of Gradient Descent and Ground Truth Predictions')\nplt.show()\n\nprint(\"Ans 3:  Comparison and Analysis\")\n# Visualization for comparison of Closed form soltution and gradient descent soltution\nplt.scatter(X_test[:, 5], y_test, color='blue', label='Actual') \nplt.scatter(X_test[:, 5], y_pred_cf, color='red', label='Predicted (Closed-Form)')\nplt.scatter(X_test[:, 5], y_pred_gd, color='orange', label='Predicted (Gradient Descent)')\nplt.xlabel('Number of Rooms (RM)')\nplt.ylabel('Median Value (MEDV)') # 'RM' (average number of rooms per dwelling)\nplt.legend()\nplt.title('Comparison of Closed-Form and Gradient Descent Predictions')\nplt.show()\n\n\n# Comparison Analysis\nprint(\"Comparison of Closed-Form Solution and Gradient Descent:\")\nprint(f\"Closed-Form Weights: {weights_cf}\")\nprint(f\"Gradient Descent Weights: {weights_gd}\")\nprint(f\"Closed-Form MSE: {mse_cf:.2f}, Gradient Descent MSE: {mse_gd:.2f}\")\nif mse_cf > mse_gd:\n    improvement_percentage = ((mse_cf - mse_gd) / mse_gd) * 100\n    print(f\"Closed-Form MSE performed better than Gradient Descent MSE by percentage of {improvement_percentage:.2f} % \")\nelif mse_cf < mse_gd:\n    improvement_percentage = ((mse_gd - mse_cf) / mse_cf) * 100\n    print(f\"Gradient DescentMSE performed better than Closed-Form MSE by percentage of {improvement_percentag:.2f} % \")\nelse:\n    print(\"Both Closed-Form MSE and Gradient Descent performed same\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T08:20:50.257956Z","iopub.execute_input":"2025-02-10T08:20:50.258302Z","iopub.status.idle":"2025-02-10T08:20:51.330111Z","shell.execute_reply.started":"2025-02-10T08:20:50.258276Z","shell.execute_reply":"2025-02-10T08:20:51.329009Z"}},"outputs":[],"execution_count":null}]}